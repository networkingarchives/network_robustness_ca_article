---
title: "Figures for Network Robustness Article published in Cultural Analytics"
---



```{r message=FALSE, warning=FALSE, include=FALSE}
library(kableExtra)
library(tidyverse)
library(tidygraph)
library(igraph)
library(DT)
library(ggraph)
library(snakecase)
library(lubridate)
library(cowplot)
```
```{r}
load('data/bcc_network')
load('data/emlo_network')
load('data/spo_network')
```

Degree distribution plots for each network:  

```{r degreeDist, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Degree distribution plot for each of the networks. The downward-sloping diagonal line on a log-log axis plot is an indication that the networks are scale-free.", fig.height=2.5, fig.width=6}
emlo_network %>% 
  distinct(X1, X2) %>% 
  graph_from_data_frame() %>% 
  as_tbl_graph() %>% 
  mutate(degree = centrality_degree(mode = 'total')) %>% 
  as_tibble() %>% 
  group_by(degree) %>% 
  tally() %>% 
  mutate(dataset = 'EMLO') %>% 
  rbind(bcc_network %>% 
          distinct(X1, X2) %>% 
          graph_from_data_frame() %>% 
          as_tbl_graph() %>% 
          mutate(degree = centrality_degree(mode = 'total')) %>%
          as_tibble() %>% 
          group_by(degree) %>% 
          tally() %>% 
          mutate(dataset = 'BCC')) %>% 
  rbind(spo_network %>% 
          distinct(X1, X2) %>% 
          graph_from_data_frame() %>% 
          as_tbl_graph() %>% 
          mutate(degree = centrality_degree(mode = 'total')) %>% 
          as_tibble() %>% 
          group_by(degree) %>% 
          tally() %>% 
          mutate(dataset = 'SPO')) %>% 
  ggplot() + 
  geom_point(aes(x = degree, y = n)) + 
  scale_x_log10() + 
  scale_y_log10() + 
  facet_wrap(~dataset, ncol = 3) + 
  theme_bw() + 
  theme(text = element_text(family = 'Times')) +
  labs(y = 'Count', x = 'Degree')



```


```{r message=FALSE, warning=FALSE, include=FALSE}

load('data/robustness_results')

```

Robustness results table:

```{r summaryTable, echo=FALSE, message=FALSE, warning=FALSE}
robustness_results %>% 
  #filter(dataset == 'emlo') %>%
  rename(variable = type, value = cor, percent_out = sample) %>% 
  mutate(metric = ifelse(metric == 'degree', 'degree_total', metric)) %>% 
  mutate(metric = ifelse(metric == 'closeness', 'close', metric)) %>%
  mutate(percent_out = as.numeric(percent_out)) %>% 
    filter(percent_out %in% c(50, 90)) %>%
    mutate(variable = ifelse(variable == 'letter_id', 'letter',
                             ifelse(variable == 'folio_or_catalogue', 'catalogue', 
                                    ifelse(variable == 'year_date', 'year', variable)))) %>% 
  rename(`Removal Type` = variable) %>% 
  group_by(metric,  `Removal Type`, percent_out) %>% 
  summarise(mean = round(mean(value),3), 
            standard_dev = round(sd(value, na.rm = T), 3)) %>% 
  ungroup() %>% 
 # select(-dataset) %>% 
  mutate(metric = ifelse(metric == 'betweenness', 'Betweenness Centrality',
                         ifelse(metric == 'close', 'Closeness Centrality',
                                ifelse(metric == 'degree_in', 'In-Degree',
                                       ifelse(metric == 'degree_out', 'Out-Degree',
                                              ifelse(metric == 'degree_total', 'Total Degree',
                                                     ifelse(metric == 'eigen', 'Eigenvector Centrality',
                                                            ifelse(metric == 'transitivity', 'Local Clustering Coefficient', metric)))))))) %>% 
  mutate(metric = ifelse(metric == 'Betweenness Centrality' & percent_out == 50, paste0(metric, footnote_marker_symbol(2)), metric)) %>%
  pivot_wider(names_from = `Removal Type`, values_from = c(mean, standard_dev)) %>% 
  select(metric, `removed` = percent_out, mean_catalogue, standard_dev_catalogue, mean_letter, standard_dev_letter, mean_nodes, standard_dev_nodes, mean_year, standard_dev_year) %>%
  kbl(escape = F, booktabs = T, col.names = c('', 'Portion removed', 'Mean $\\rho$', paste0('SD',footnote_marker_symbol(1)), 'Mean $\\rho$', 'SD', 'Mean $\\rho$', 'SD', 'Mean $\\rho$', 'SD'), caption = "Summary statistics for the removal correlations at fifty and ninety percent removed, showing the mean Spearman's rho ($\\rho$) and standard deviation across forty iterations.") %>% 
  add_header_above(c(" " = 2, "Catalogues" = 2, "Letters" = 2, "Nodes" = 2, "Years" = 2)) %>% 
  kable_styling(latex_options = c("striped", 'scale_down')) %>% footnote(symbol = c('Standard deviation', 'Explanations of this and other standard network terms can be found in a glossary at the end of this article.'))
```

## Letter Removal


```{r lettersRemoval, echo=FALSE, message=FALSE, warning=FALSE, fig.align= 'center', fig.width = 6, fig.height=4, fig.cap="Robustness of Key Measures to Letter Removal. The blue line indicates the mean Spearman correlation ($\\rho$) mean across all forty iterations. The standard deviation at each sample is represented as a shaded gray area, however it is usually not visible in this mode of removal because the variation between iterations was so small. Full results are in the appendix."}
robustness_results %>% 
 # filter(dataset == 'emlo') %>% 
  mutate(metric = ifelse(metric == 'degree' | metric == 'degree_total', 'Degree', metric)) %>%
  mutate(metric = ifelse(metric == 'eigen', 'Eigenvector', metric)) %>%
  mutate(metric = ifelse(metric == 'transitivity', 'Transitivity', metric)) %>% 
  mutate(metric = ifelse(metric == 'closeness', 'Closeness', metric))%>% 
  mutate(metric = ifelse(metric == 'betweenness', 'Betweenness', metric)) %>%
  mutate(sample = as.numeric(sample)) %>% 
  rename(variable = type) %>% 
    mutate(variable = ifelse(variable == 'letter_id', 'letter',
                             ifelse(variable == 'folio_or_catalogue', 'catalogue', 
                                    ifelse(variable == 'year_date', 'year', variable)))) %>% 
  mutate(dataset = ifelse(dataset == 'emlo', 'EMLO',
                          ifelse(dataset == 'bcc', 'BCC', 'SPO')))%>% 
  rename(`Removal Type` = variable) %>%
  filter(`Removal Type` == 'letter') %>% 
  filter(metric %in% c('Degree', 'Eigenvector', 'Transitivity','Betweenness')) %>%  
  group_by(metric, dataset, `Removal Type`, sample) %>% 
  summarise(mean = mean(cor), min = min(cor), max = max(cor),sd = sd(cor)) %>% 
  ggplot() + 
  geom_ribbon(aes(x = sample, ymax = mean+sd, ymin = mean-sd), alpha = .6, fill = 'gray75')  + 
   geom_line(aes(x = sample, y = mean), alpha = .8, color = 'blue') +
  labs(x = 'Percentage Removed (letters)', y = expression(rho*" (mean)")) + 
  theme_bw() +
  coord_fixed(100,ylim = c(0,1)) + 
  facet_grid(dataset~metric) + 
  scale_color_viridis_d()  +
  scale_y_continuous(breaks = c(.5, 1)) +
  scale_x_continuous(breaks = c(50, 100)) + 
  theme(panel.grid.minor =element_blank(), 
        strip.text = element_text(size = 11), 
        text = element_text(family = 'Times'))

```

## Node Removal



```{r nodesRemoval, echo=FALSE, message=FALSE, warning=FALSE, fig.align= 'center', fig.width = 6, fig.height=4,fig.cap="Robustness of Measures to Node Removal. The blue line indicates the mean Spearman correlation ($\\rho$) across forty iterations. The standard deviation at each sample is represented as a shaded gray area. Generally, correlations remained high until 70\\% of the network was removed and then declined sharply. Degree and betweeness sensitivity were similar to letter removal but closeness (not shown) and eigenvector centralities displayed more variation. Full results in the appendix."}
robustness_results %>% 
 # filter(dataset == 'emlo') %>% 
  mutate(metric = ifelse(metric == 'degree' | metric == 'degree_total', 'Degree', metric)) %>%
  mutate(metric = ifelse(metric == 'eigen', 'Eigenvector', metric)) %>%
  mutate(metric = ifelse(metric == 'transitivity', 'Transitivity', metric)) %>% 
  mutate(metric = ifelse(metric == 'closeness', 'Closeness', metric))%>% 
  mutate(metric = ifelse(metric == 'betweenness', 'Betweenness', metric)) %>%
  mutate(sample = as.numeric(sample)) %>% 
  rename(variable = type) %>% 
    mutate(variable = ifelse(variable == 'letter_id', 'letter',
                             ifelse(variable == 'folio_or_catalogue', 'catalogue', 
                                    ifelse(variable == 'year_date', 'year', variable)))) %>% 
  mutate(dataset = ifelse(dataset == 'emlo', 'EMLO',
                          ifelse(dataset == 'bcc', 'BCC', 'SPO')))%>% 
  rename(`Removal Type` = variable) %>%
  filter(`Removal Type` == 'nodes') %>% 
  filter(metric %in% c('Degree', 'Eigenvector', 'Transitivity','Betweenness')) %>%  
  group_by(metric, dataset, `Removal Type`, sample) %>% 
  summarise(mean = mean(cor), min = min(cor), max = max(cor),sd = sd(cor)) %>% 
  ggplot() + 
  geom_ribbon(aes(x = sample, ymax = mean+sd, ymin = mean-sd), alpha = .6, fill = 'gray75')  + 
   geom_line(aes(x = sample, y = mean), alpha = .8, color = 'blue') +
  labs(x = 'Percentage Removed (nodes)', y = expression(rho*" (mean)")) + 
  theme_bw() +
  coord_fixed(100,ylim = c(0,1)) + 
  facet_grid(dataset~metric) + 
  scale_color_viridis_d()  +
  scale_y_continuous(breaks = c(.5, 1)) +
  scale_x_continuous(breaks = c(50, 100)) + 
  theme(panel.grid.minor =element_blank(), 
        strip.text = element_text(size = 11), 
        text = element_text(family = 'Times'))



```

## Catalogue removal


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align= 'center', fig.width = 6, fig.height=2,fig.cap="Robustness of Measures to Catalogue Removal in EMLO dataset. Only EMLO has the catalogue as an organisational unit and so is displayed separately here. Removing full catalogues still did not have a large effect on correlations, though more variation between iterations was displayed - most likely because catalogue vary by size.  The blue line indicates the mean Spearman correlation ($\\rho$) across all forty iterations. The standard deviation at each sample is represented as a shaded gray area. Full results in the appendix."}
robustness_results %>% 
  filter(dataset == 'emlo') %>% 
  mutate(metric = ifelse(metric == 'degree' | metric == 'degree_total', 'Degree', metric)) %>%
  mutate(metric = ifelse(metric == 'eigen', 'Eigenvector', metric)) %>%
  mutate(metric = ifelse(metric == 'transitivity', 'Transitivity', metric)) %>% 
  mutate(metric = ifelse(metric == 'closeness', 'Closeness', metric))%>% 
  mutate(metric = ifelse(metric == 'betweenness', 'Betweenness', metric)) %>%
  mutate(sample = as.numeric(sample)) %>% 
  rename(variable = type) %>% 
    mutate(variable = ifelse(variable == 'letter_id', 'letter',
                             ifelse(variable == 'folio_or_catalogue', 'catalogue', 
                                    ifelse(variable == 'year_date', 'year', variable)))) %>% 
  mutate(dataset = ifelse(dataset == 'emlo', 'EMLO',
                          ifelse(dataset == 'bcc', 'BCC', 'SPO')))%>% 
  rename(`Removal Type` = variable) %>%
  filter(`Removal Type` == 'catalogue') %>% 
  filter(metric %in% c('Degree', 'Eigenvector', 'Transitivity','Betweenness')) %>%  
  group_by(metric, dataset, `Removal Type`, sample) %>% 
  summarise(mean = mean(cor), min = min(cor), max = max(cor),sd = sd(cor)) %>% 
  ggplot() + 
  geom_ribbon(aes(x = sample, ymax = mean+sd, ymin = mean-sd), alpha = .6, fill = 'gray75')  + 
   geom_line(aes(x = sample, y = mean), alpha = .8, color = 'blue') +
  labs(x = 'Percentage Removed (catalogues)', y = expression(rho*" (mean)")) + 
  theme_bw() +
  coord_fixed(100,ylim = c(0,1)) + 
  facet_grid(dataset~metric) + 
  scale_color_viridis_d()  +
  scale_y_continuous(breaks = c(.5, 1)) +
  scale_x_continuous(breaks = c(50, 100)) + 
  theme(panel.grid.minor =element_blank(), 
        strip.text = element_text(size = 11), 
        text = element_text(family = 'Times'))



```

## Folio Removal



```{r folio, echo=FALSE, message=FALSE, warning=FALSE, fig.align= 'center', fig.width = 6, fig.height=3, fig.cap="Folio Removal results for BCC and SPO datasets. Blue line indicates the mean correlation score across all forty iterations. The standard deviation at each sample is represented as a shaded gray area. SPO and BCC include information on the source folio, and we used this as a unit to remove in sampling. Results differed as a result of the type of material collected in these folios: BCC folios are often person-based, and therefore show more variability, whereas SPO folios are often archived by subject or chronologically, and seem to display less variability."}
robustness_results %>% 
  filter(dataset != 'emlo') %>% 
  mutate(metric = ifelse(metric == 'degree' | metric == 'degree_total', 'Degree', metric)) %>%
  mutate(metric = ifelse(metric == 'eigen', 'Eigenvector', metric)) %>%
  mutate(metric = ifelse(metric == 'transitivity', 'Transitivity', metric)) %>% 
  mutate(metric = ifelse(metric == 'closeness', 'Closeness', metric))%>% 
  mutate(metric = ifelse(metric == 'betweenness', 'Betweenness', metric)) %>%
  mutate(sample = as.numeric(sample)) %>% 
  rename(variable = type) %>% 
    mutate(variable = ifelse(variable == 'letter_id', 'letter',
                             ifelse(variable == 'folio_or_catalogue', 'catalogue', 
                                    ifelse(variable == 'year_date', 'year', variable)))) %>% 
  mutate(dataset = ifelse(dataset == 'emlo', 'EMLO',
                          ifelse(dataset == 'bcc', 'BCC', 'SPO')))%>% 
  rename(`Removal Type` = variable) %>%
  filter(`Removal Type` == 'catalogue') %>% 
  filter(metric %in% c('Degree', 'Eigenvector', 'Transitivity','Betweenness')) %>%  
  group_by(metric, dataset, `Removal Type`, sample) %>% 
  summarise(mean = mean(cor), min = min(cor), max = max(cor),sd = sd(cor)) %>% 
  ggplot() + 
  geom_ribbon(aes(x = sample, ymax = mean+sd, ymin = mean-sd), alpha = .6, fill = 'gray75')  + 
   geom_line(aes(x = sample, y = mean), alpha = .8, color = 'blue') +
  labs(x = 'Percentage Removed (folios)', y = expression(rho*" (mean)")) + 
  theme_bw() +
  coord_fixed(100,ylim = c(0,1)) + 
  facet_grid(dataset~metric) + 
  scale_color_viridis_d()  +
  scale_y_continuous(breaks = c(.5, 1)) +
  scale_x_continuous(breaks = c(50, 100)) + 
  theme(panel.grid.minor =element_blank(), 
        strip.text = element_text(size = 11), 
        text = element_text(family = 'Times'))



```

## Year removal


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align= 'center',fig.width = 6, fig.height=4, fig.cap="Robustness of Measures to Year Removal. The blue line indicates the mean Spearman correlation score across all forty iterations. The standard deviation at each sample is represented as a shaded gray area. The results mirrored those of letter removal, with some more variability due to variations in the amount of material per year. Full results in the appendix)"}
robustness_results %>% 
 # filter(dataset == 'emlo') %>% 
  mutate(metric = ifelse(metric == 'degree' | metric == 'degree_total', 'Degree', metric)) %>%
  mutate(metric = ifelse(metric == 'eigen', 'Eigenvector', metric)) %>%
  mutate(metric = ifelse(metric == 'transitivity', 'Transitivity', metric)) %>% 
  mutate(metric = ifelse(metric == 'closeness', 'Closeness', metric))%>% 
  mutate(metric = ifelse(metric == 'betweenness', 'Betweenness', metric)) %>%
  mutate(sample = as.numeric(sample)) %>% 
  rename(variable = type) %>% 
    mutate(variable = ifelse(variable == 'letter_id', 'letter',
                             ifelse(variable == 'folio_or_catalogue', 'catalogue', 
                                    ifelse(variable == 'year_date', 'year', variable)))) %>% 
  mutate(dataset = ifelse(dataset == 'emlo', 'EMLO',
                          ifelse(dataset == 'bcc', 'BCC', 'SPO')))%>% 
  rename(`Removal Type` = variable) %>%
  filter(`Removal Type` == 'year') %>% 
  filter(metric %in% c('Degree', 'Eigenvector', 'Transitivity','Betweenness')) %>%  
  group_by(metric, dataset, `Removal Type`, sample) %>% 
  summarise(mean = mean(cor), min = min(cor), max = max(cor),sd = sd(cor)) %>% 
  ggplot() + 
  geom_ribbon(aes(x = sample, ymax = mean+sd, ymin = mean-sd), alpha = .6, fill = 'gray75')  + 
   geom_line(aes(x = sample, y = mean), alpha = .8, color = 'blue') +
  labs(x = 'Percentage Removed (years)', y = expression(rho*" (mean)")) + 
  theme_bw() +
  coord_fixed(100,ylim = c(0,1)) + 
  facet_grid(dataset~metric) + 
  scale_color_viridis_d()  +
  scale_y_continuous(breaks = c(.5, 1)) +
  scale_x_continuous(breaks = c(50, 100)) + 
  theme(panel.grid.minor =element_blank(), 
        strip.text = element_text(size = 11), 
        text = element_text(family = 'Times'))

```

## Reconcilation and Disambiguation Errors    



```{r disambigResults, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5, fig.width=6, fig.cap = "Robustness results from disambiguation error experiment. To calculate the impact of no or unfinished data cleaning, we reverted progressively larger samples of the data to its uncleaned state, using the log files generated by the data cleaning process. Results were similar to other methods of sampling. In (b), a noticeable difference between in/out degree and total degree robustness can be seen. This is because most of the data cleaning is merging, and often the secondary name variants are mostly either in or outgoing letters rather than a balance between the two."}
library(cowplot)
load('data/samples_x')

a = samples %>% filter(metric %in% c('degree_in', 'degree_out', 'degree_total')) %>% 
  mutate(removed = 1-removed) %>% 
  mutate(removed = as.character(removed)) %>% 
  ggplot() + 
  geom_point(aes(x =removed, 
                   y = cor, color = metric)) +
  coord_cartesian(ylim = c(0.5,1)) + 
  scale_y_continuous(breaks = c(.5, 1))+ 
  theme_bw() +  theme(panel.grid.minor =element_blank(), 
                      strip.text = element_text(size = 11), 
                      text = element_text(family = 'Times'))+ 
  theme(legend.position = 'bottom') + guides(color = guide_legend(nrow = 2))

b = samples %>% 
  mutate(removed = 1-removed) %>% 
  mutate(removed = as.character(removed)) %>% 
  ggplot() + 
  geom_boxplot(aes(x =removed, 
                   y = cor, group = NULL), alpha = .4)+
  coord_cartesian(ylim = c(0,1)) + 
  scale_y_continuous(breaks = c(.5, 1))+ 
  theme_bw() +  theme(panel.grid.minor =element_blank(), 
                      strip.text = element_text(size = 11), 
                      text = element_text(family = 'Times', size = 8)) + 
  facet_wrap(~metric) 



plot_grid(b,a, nrow = 1, labels = c('(a)', '(b)'), 
          label_fontfamily = 'Times', rel_widths = c(.6, .4),
  label_size = 12, hjust = 0)

```


# Effect on real-world results

```{r message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
# Generate some new robustness results at 20, 50 and 80% removed: 

# network_filtered_f = spo_raw %>% 
#   dplyr::select(X1, X2, X3, X8) %>% 
#   mutate(X8 = str_remove(X8, "\\sf\\.[0-9]{1,}")) %>% 
#   mutate(year_date = year(ymd(X3))) %>% 
#   mutate(folio = str_remove(X8, "f\\.[0-9]{1}")) %>% 
#   filter(!is.na(folio)) %>% 
#   mutate(folio = trimws(folio, which = 'both'))
# results_80 = list()
# for(i in 1:100){
#   
# sample_edges = network_filtered_f %>%
# sample_frac((100-20)/100, replace = F) %>%
# distinct(.[1], .[2])
# 
# sub.graphmatr = graph_from_data_frame(sample_edges)
# 
# 
# d = sub.graphmatr %>% as_tbl_graph() %>% 
#   mutate(degree = centrality_degree(mode = 'total')) %>% 
#   mutate(betweenness = centrality_betweenness()) %>% 
#   mutate(d_b = betweenness/degree) %>% 
#   filter(degree>10) %>% 
#   as_tibble() %>%
#   mutate(run = i)
# 
# results_80[[i]] = d
# 
# }
# 
# library(data.table)
# 
# results_80 = results_80 %>% rbindlist()
# 
# results_80 =results_80 %>% 
#   group_by(run) %>% 
#   mutate(rank_degree = rank(-degree)) %>% 
#   mutate(rank_between = rank(-betweenness))  %>% 
#   mutate(rank_d_b = rank(-d_b)) %>% group_by(name) %>%
#   mutate(min_rank = min(rank_d_b))
# 
# 
# 
# results_50 = list()
# for(i in 1:100){
#   
# sample_edges = network_filtered_f %>%
# sample_frac((100-50)/100, replace = F) %>%
# distinct(.[1], .[2])
# 
# sub.graphmatr = graph_from_data_frame(sample_edges)
# 
# 
# d = sub.graphmatr %>% as_tbl_graph() %>% 
#   mutate(degree = centrality_degree(mode = 'total')) %>% 
#   mutate(betweenness = centrality_betweenness()) %>% 
#   mutate(d_b = betweenness/degree) %>% 
#   filter(degree>10) %>% 
#   as_tibble() %>%
#   mutate(run = i)
# 
# results_50[[i]] = d
# 
# }
# 
# 
# 
# results_50 = results_50 %>% rbindlist()
# 
# results_50 =results_50 %>% 
#   group_by(run) %>% 
#   mutate(rank_degree = rank(-degree)) %>% 
#   mutate(rank_between = rank(-betweenness))  %>% 
#   mutate(rank_d_b = rank(-d_b)) %>% group_by(name) %>%
#   mutate(min_rank = min(rank_d_b))
# 
# results_20 = list()
# for(i in 1:100){
#   
# sample_edges = network_filtered_f %>%
# sample_frac((100-80)/100, replace = F) %>%
# distinct(.[1], .[2])
# 
# sub.graphmatr = graph_from_data_frame(sample_edges)
# 
# 
# d = sub.graphmatr %>% as_tbl_graph() %>% 
#   mutate(degree = centrality_degree(mode = 'total')) %>% 
#   mutate(betweenness = centrality_betweenness()) %>% 
#   mutate(d_b = betweenness/degree) %>% 
#   filter(degree>10) %>% 
#   as_tibble() %>%
#   mutate(run = i)
# 
# results_20[[i]] = d
# 
# }
# 
# library(data.table)
# 
# results_20 = results_20 %>% rbindlist()
# 
# results_20 =results_20 %>% 
#   group_by(run) %>% 
#   mutate(rank_degree = rank(-degree)) %>% 
#   mutate(rank_between = rank(-betweenness))  %>% 
#   mutate(rank_d_b = rank(-d_b)) %>% group_by(name) %>%
#   mutate(min_rank = min(rank_d_b))
# 


# Load pre-generated results: 

load('data/results_20')
load('data/results_50')
load('data/results_80')



sec_of_state_ranks = rbind(results_50 %>% 
                                    mutate(amount = '50% Remaining'), results_80%>% 
                                    mutate(amount = '80% Remaining'), results_20 %>% 
                             mutate(amount = '20% Remaining')) %>% filter( name%in% c('42999', '18912','41287'))

sec_of_state_ranks$f_amount = factor(sec_of_state_ranks$amount, levels = c('80% Remaining', '50% Remaining','20% Remaining'))

```

Figure for this: 

```{r secOfState, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="To demonstrate the effect of missing data on individual network results, we calculated the degree ranks of three Secretaries of State at 20, 50 and 80 per cent of the network remaining, run with 100 random samples. The rankings of all three stayed remarkably similar, even with only 20% of letters remaining.", cache = TRUE, fig.height=3, fig.width=4}




p = ggplot() + 
  geom_point(data =sec_of_state_ranks, aes(x = run, y = rank_degree, color = name), alpha = .8) + facet_wrap(~f_amount) + scale_y_reverse(breaks = 1:15)+
  theme_bw()+ theme(legend.position = 'bottom') +labs(color = NULL) + guides(color = guide_legend(nrow = 2))
  
p+ theme(text = element_text(family = 'Times'))

```

## Degree-Betweenness plot assessment:  

```{r degreeBetween, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Assessing the effect of data loss on a downstream task. James Butler, Earl of Ormond can be described as a 'bridge' node in State Papers Online: neither his degree or betweenness centrality scores are within the highest-ranking, but his betweenness centrality is proportionately high in comparison to his degree. This can be seen from his position well into the lower half in a scatterplot of the rankings of the two measurements in (a), which plot the rank of his betweenness centrality against his degree, on a log-log plot. In (b), (c) and (d) we calculated his position with 20%, 50% and 80% of the network removed, respectively. This was done 100 times and the results plotted one over another, with Ormond highlighted. In almost all cases, as can be seen here, he remains roughly in the same area of the scatterplot.",fig.width = 6, fig.height=4}

library(cowplot)
set.seed(1234)
spo_raw = read_delim('../../../Documents/MOST RECENT DATA/fromto_all_place_mapped_stuart_sorted', delim = '\t', col_names = F)
network_filtered = spo_raw %>% 
  dplyr::select(X1, X2, X3, X8) %>% 
  mutate(X8 = str_remove(X8, "\\sf\\.[0-9]{1,}")) %>% 
  mutate(year_date = year(ymd(X3)))

network_filtered_f = spo_raw %>% 
  dplyr::select(X1, X2, X3, X8) %>% 
  mutate(X8 = str_remove(X8, "\\sf\\.[0-9]{1,}")) %>% 
  mutate(year_date = year(ymd(X3))) %>% 
  mutate(folio = str_remove(X8, "f\\.[0-9]{1}")) %>% 
  filter(!is.na(folio)) %>% 
  mutate(folio = trimws(folio, which = 'both'))

nsim =1
graphmatr = graph_from_data_frame(network_filtered_f %>%
                                    distinct(.[1], .[2]), directed = T)

nodes = graphmatr %>% 
  as_tbl_graph() %>% 
  mutate(degree =centrality_degree(mode = 'total')) %>% 
 # filter(degree>1) %>%
  activate(nodes) %>% as_tibble() %>% pull(name)

network_filtered_f = spo_raw %>% 
  dplyr::select(X1, X2, X3, X8) %>% 
  mutate(X8 = str_remove(X8, "\\sf\\.[0-9]{1,}")) %>% 
  mutate(year_date = year(ymd(X3))) %>% 
  mutate(folio = str_remove(X8, "f\\.[0-9]{1}")) %>% 
  filter(!is.na(folio)) %>% 
  mutate(folio = trimws(folio, which = 'both')) %>% 
  filter(X1 %in% nodes |X2 %in% nodes)

graphmatr = graph_from_data_frame(network_filtered_f %>%
distinct(.[1], .[2]), directed = T)

all_ranks = rbind(results_50 %>% 
                                    mutate(amount = '50% Remaining'), results_80%>% 
                                    mutate(amount = '80% Remaining'), results_20 %>% 
                             mutate(amount = '20% Remaining')) %>% mutate(butler = ifelse( name == '20751', 'yes', 'no'))


full_results_for_butler = network_filtered_f %>% distinct(X1, X2) %>% graph_from_data_frame() %>% as_tbl_graph() %>% 
  mutate(degree = centrality_degree(mode = 'total')) %>% 
  mutate(betweenness = centrality_betweenness()) %>% 
  mutate(d_b = betweenness/degree)  %>% 
  filter(degree>10)%>% 
  as_tibble()%>% 
  mutate(rank_degree = rank(-degree)) %>% 
  mutate(rank_between = rank(-betweenness))  %>% 
  mutate(rank_d_b = rank(-d_b)) %>% group_by(name) %>%
  mutate(min_rank = min(rank_d_b))


  a = ggplot(data = full_results_for_butler %>% 
  mutate(butler = ifelse( name == '20751', 'yes', 'no'))) + 
  geom_point(aes(x = degree, 
                 y = rank_between, 
                 color = butler, alpha = butler)) + 
  scale_alpha_manual(values = c(.3, .9))  +
  annotate("segment", x = 56, xend = 56, y = 3, yend = 30,
           colour = "black", size =.3, arrow = arrow(length = unit(0.05, "inches"),
      ends = "last", type = "closed")) +
  annotate("text", x = 56,  y = 2, label = 'James Butler',
           colour = "black", size = 2.5)+ theme_bw() + theme(legend.position = 'none') + scale_x_log10(limits = c(10,2336)) + scale_y_log10()  + coord_cartesian() + theme(text = element_text(family = 'Times'))

b = results_80 %>% mutate(butler = ifelse( name == '20751', 'yes', 'no')) %>% 
  ggplot() + 
  geom_point(aes(x = degree, 
                 y = rank_between, 
                 group = run, 
                 color = butler, alpha = butler))+
  scale_alpha_manual(values = c(.01, .8)) + theme_bw() + theme(legend.position = 'none') + scale_x_log10(limits = c(10,2336)) + scale_y_log10()   + theme(text = element_text(family = 'Times'))

c = results_50 %>% mutate(butler = ifelse( name == '20751', 'yes', 'no')) %>% 
  ggplot() + 
  geom_point(aes(x = degree, 
                 y = rank_between, 
                 group = run, 
                 color = butler, alpha = butler))+ 
  scale_alpha_manual(values = c(.01, .8)) + theme_bw() + 
  theme(legend.position = 'none') + 
  scale_x_log10(limits = c(10,2336)) + scale_y_log10()  +
  theme(text = element_text(family = 'Times'))

d = results_20 %>% mutate(butler = ifelse( name == '20751', 'yes', 'no')) %>% 
  ggplot() + 
  geom_point(aes(x = degree, 
                 y = rank_between, 
                 group = run, 
                 color = butler, alpha = butler)) + 
  scale_alpha_manual(values = c(.01, .8)) + theme_bw() + 
  theme(legend.position = 'none') +
  scale_x_log10(limits = c(10,2336)) + scale_y_log10()  + 
  theme(text = element_text(family = 'Times'))

plot_grid(a,b,c,d, labels = c('a', 'b', 'c', 'd'),label_fontface = "bold", label_size = 12, label_fontfamily = 'Times')



```


# Appendix:

Additional results produced by the robustness analysis (including those used above), including in-degree, out-degree, and closeness centrality. As in the main plots, blue line indicates the mean and the limits of the gray area the standard deviation.

## Full Results - Early Modern Letters Online (EMLO)

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height = 8, fig.width=12}
robustness_results %>% 
  mutate(metric = str_remove(metric, "original_catalogue_name_|none_|year_date_|nodes_")) %>%
  mutate(metric = ifelse(metric == 'degree' | metric == 'degree_total', 'Degree', metric)) %>%
  mutate(metric = ifelse(metric == 'eigen', 'Eigenvector', metric)) %>%
  mutate(metric = ifelse(metric == 'transitivity', 'Transitivity', metric)) %>% 
  mutate(metric = ifelse(metric == 'closeness', 'close', metric)) %>% 
  mutate(metric = ifelse(metric == 'betweenness', 'Betweenness', metric))%>% 
  mutate(metric = ifelse(metric == 'close', 'Closeness', metric))%>% 
  mutate(metric = ifelse(metric == 'degree_in', 'In-degree', metric)) %>%
  mutate(metric = ifelse(metric == 'degree_out', 'Out-degree', metric)) %>%
  as_tibble() %>% 
  mutate(instance = 1:nrow(.)) %>% 
    mutate(type = ifelse(type == 'letter_id', 'letter',
                             ifelse(type == 'folio_or_catalogue', 'catalogue', 
                                    ifelse(type == 'folio', 'catalogue/folio',
                                           ifelse(type == 'year_date', 'year', type))))) %>% 
  mutate(dataset = ifelse(dataset == 'emlo', 'EMLO',
                          ifelse(dataset == 'bcc', 'BCC', 'SPO')))%>% 
  filter(dataset == 'EMLO') %>% 
  rename(`Removal Type` = type) %>%
 # filter(`Removal Type` == 'letter') %>% 
 # filter(metric %in% c('Degree', 'Eigenvector', 'Transitivity','Betweenness')) %>%  
  group_by(metric, dataset, `Removal Type`, sample) %>% 
  summarise(mean = mean(cor), min = min(cor), max = max(cor),sd = sd(cor)) %>% 
  ggplot() + 
  geom_ribbon(aes(x = sample, ymax = mean+sd, ymin = mean-sd), alpha = .6, fill = 'gray75')  + 
   geom_line(aes(x = sample, y = mean), alpha = .8, color = 'blue') +
  labs(x = 'Percentage Removed', y = "Spearman's Rho") + 
  theme_bw() +
  coord_fixed(100,ylim = c(0,1)) + 
  facet_grid(dataset~`Removal Type`~metric) + 
  scale_color_viridis_d()  +
  scale_y_continuous(breaks = c(.5, 1))  +
  scale_x_continuous(breaks = c(50, 100)) + 
  theme(panel.grid.minor =element_blank(), 
        strip.text = element_text(size = 16), 
        text = element_text(family = 'Times', size = 14))
```

\newpage

## Full Results - State Papers Online (SPO)

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height = 8, fig.width=12}
robustness_results %>% 
  mutate(metric = str_remove(metric, "original_catalogue_name_|none_|year_date_|nodes_")) %>%
  mutate(metric = ifelse(metric == 'degree' | metric == 'degree_total', 'Degree', metric)) %>%
  mutate(metric = ifelse(metric == 'eigen', 'Eigenvector', metric)) %>%
  mutate(metric = ifelse(metric == 'transitivity', 'Transitivity', metric)) %>% 
  mutate(metric = ifelse(metric == 'closeness', 'close', metric)) %>% 
  mutate(metric = ifelse(metric == 'betweenness', 'Betweenness', metric))%>% 
  mutate(metric = ifelse(metric == 'close', 'Closeness', metric))%>% 
  mutate(metric = ifelse(metric == 'degree_in', 'In-degree', metric)) %>%
  mutate(metric = ifelse(metric == 'degree_out', 'Out-degree', metric)) %>%
  as_tibble() %>% 
  mutate(instance = 1:nrow(.)) %>% 
    mutate(type = ifelse(type == 'letter_id', 'letter',
                             ifelse(type == 'original_catalogue_name', 'catalogue/folio', 
                                    ifelse(type == 'folio_or_catalogue', 'folio',
                                           ifelse(type == 'year_date', 'year', type))))) %>% 
  mutate(dataset = ifelse(dataset == 'emlo', 'EMLO',
                          ifelse(dataset == 'bcc', 'BCC', 'SPO')))%>% 
  rename(`Removal Type` = type) %>%
  filter(dataset == 'SPO') %>% 
 # filter(`Removal Type` == 'letter') %>% 
 # filter(metric %in% c('Degree', 'Eigenvector', 'Transitivity','Betweenness')) %>%  
  group_by(metric, dataset, `Removal Type`, sample) %>% 
  summarise(mean = mean(cor), min = min(cor), max = max(cor),sd = sd(cor)) %>% 
  ggplot() + 
  geom_ribbon(aes(x = sample, ymax = mean+sd, ymin = mean-sd), alpha = .6, fill = 'gray75')  + 
   geom_line(aes(x = sample, y = mean), alpha = .8, color = 'blue') +
  labs(x = 'Percentage Removed', y = "Spearman's Rho") + 
  theme_bw() +
  coord_fixed(100,ylim = c(0,1)) + 
  facet_grid(dataset~`Removal Type`~metric) + 
  scale_color_viridis_d()  +
  scale_y_continuous(breaks = c(.5, 1))  +
  scale_x_continuous(breaks = c(50, 100)) + 
  theme(panel.grid.minor =element_blank(), 
        strip.text = element_text(size = 16), 
        text = element_text(family = 'Times', size = 14))
```

\newpage

## Full Results - Bodleian Card Catalogue (BCC)

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height = 8, fig.width=12}
robustness_results %>% 
  mutate(metric = str_remove(metric, "original_catalogue_name_|none_|year_date_|nodes_")) %>%
  mutate(metric = ifelse(metric == 'degree' | metric == 'degree_total', 'Degree', metric)) %>%
  mutate(metric = ifelse(metric == 'eigen', 'Eigenvector', metric)) %>%
  mutate(metric = ifelse(metric == 'transitivity', 'Transitivity', metric)) %>% 
  mutate(metric = ifelse(metric == 'closeness', 'close', metric)) %>% 
  mutate(metric = ifelse(metric == 'betweenness', 'Betweenness', metric))%>% 
  mutate(metric = ifelse(metric == 'close', 'Closeness', metric))%>% 
  mutate(metric = ifelse(metric == 'degree_in', 'In-degree', metric)) %>%
  mutate(metric = ifelse(metric == 'degree_out', 'Out-degree', metric)) %>%
  as_tibble() %>% 
  mutate(instance = 1:nrow(.)) %>% 
    mutate(type = ifelse(type == 'letter_id', 'letter',
                             ifelse(type == 'original_catalogue_name', 'catalogue/folio', 
                                    ifelse(type == 'folio_or_catalogue', 'folio',
                                           ifelse(type == 'year_date', 'year', type))))) %>% 
  mutate(dataset = ifelse(dataset == 'emlo', 'EMLO',
                          ifelse(dataset == 'bcc', 'BCC', 'SPO'))) %>% 
  rename(`Removal Type` = type) %>%
  filter(dataset == 'BCC') %>% 
 # filter(`Removal Type` == 'letter') %>% 
 # filter(metric %in% c('Degree', 'Eigenvector', 'Transitivity','Betweenness')) %>%  
  group_by(metric, dataset, `Removal Type`, sample) %>% 
  summarise(mean = mean(cor), min = min(cor), max = max(cor),sd = sd(cor)) %>% 
  ggplot() + 
  geom_ribbon(aes(x = sample, ymax = mean+sd, ymin = mean-sd), alpha = .6, fill = 'gray75')  + 
   geom_line(aes(x = sample, y = mean), alpha = .8, color = 'blue') +
  labs(x = 'Percentage Removed', y = "Spearman's Rho") + 
  theme_bw() +
  coord_fixed(100,ylim = c(0,1)) + 
  facet_grid(dataset~`Removal Type`~metric) + 
  scale_color_viridis_d()  +
  scale_y_continuous(breaks = c(.5, 1))  +
  scale_x_continuous(breaks = c(50, 100)) + 
  theme(panel.grid.minor =element_blank(), 
        strip.text = element_text(size = 16), 
        text = element_text(family = 'Times', size = 14))
```

## Sample results for dataset showing that the relationship between the original and sampled dataset is monotonic. 

```{r example, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Two example comparisons of each centrality score, with 80% and 50% of letters removed at random, displaying a monotonic relationship between the data and therefore suitable for Spearman's rank correlation. Even with large numbers of letters removed, Spearman's rho value remains high", fig.height=8, fig.width=3.5}


e_bw.g <- igraph::eigen_centrality(graphmatr)['vector'][[1]]
d_bw.g <- igraph::degree(graphmatr, mode = 'total')
d_i_bw.g <- igraph::degree(graphmatr, mode = 'in')
d_o_bw.g <- igraph::degree(graphmatr, mode = 'out')
b_bw.g <- igraph::betweenness(graphmatr)


sample_edges = network_filtered_f %>%
sample_frac((100-50)/100, replace = F) %>%
distinct(.[1], .[2])

sub.graphmatr = graph_from_data_frame(sample_edges)


e_temp.stats_50 = igraph::eigen_centrality(sub.graphmatr)['vector'][[1]] %>% tibble::enframe()
e_temp.stats_50$full = e_bw.g[e_temp.stats_50$name]
e_temp.stats_50$metric = 'eigen'

d_temp.stats_50 = igraph::degree(sub.graphmatr, mode = 'total') %>% tibble::enframe()
d_temp.stats_50$full = d_bw.g[d_temp.stats_50$name]
d_temp.stats_50$metric = 'degree-total'

d_i_temp.stats_50 = igraph::degree(sub.graphmatr, mode = 'in') %>% tibble::enframe()
d_i_temp.stats_50$full = d_i_bw.g[d_i_temp.stats_50$name]
d_i_temp.stats_50$metric = 'degree-in'

d_o_temp.stats_50 = igraph::degree(sub.graphmatr, mode = 'out') %>% tibble::enframe()
d_o_temp.stats_50$full = d_o_bw.g[d_o_temp.stats_50$name]
d_o_temp.stats_50$metric = 'degree-out'

b_temp.stats_50 = igraph::betweenness(sub.graphmatr) %>% tibble::enframe()
b_temp.stats_50$full = b_bw.g[b_temp.stats_50$name]
b_temp.stats_50$metric = 'betweenness'



graphmatr = graph_from_data_frame(network_filtered_f %>%
                                    distinct(.[1], .[2]), directed = T)

e_bw.g <- igraph::eigen_centrality(graphmatr)['vector'][[1]]
d_bw.g <- igraph::degree(graphmatr, mode = 'total')
d_i_bw.g <- igraph::degree(graphmatr, mode = 'in')
d_o_bw.g <- igraph::degree(graphmatr, mode = 'out')
b_bw.g <- igraph::betweenness(graphmatr)

sample_edges = network_filtered_f %>%
  sample_frac((100-90)/100, replace = F) %>%
  distinct(.[1], .[2])

sub.graphmatr = graph_from_data_frame(sample_edges)

e_temp.stats_20 = igraph::eigen_centrality(sub.graphmatr)['vector'][[1]] %>% tibble::enframe()
e_temp.stats_20$full = e_bw.g[e_temp.stats_20$name]
e_temp.stats_20$metric = 'eigen'

d_temp.stats_20 = igraph::degree(sub.graphmatr, mode = 'total') %>% tibble::enframe()
d_temp.stats_20$full = d_bw.g[d_temp.stats_20$name] 
d_temp.stats_20$metric = 'degree-total'

d_i_temp.stats_20 = igraph::degree(sub.graphmatr, mode = 'in') %>% tibble::enframe()
d_i_temp.stats_20$full = d_i_bw.g[d_i_temp.stats_20$name]
d_i_temp.stats_20$metric = 'degree-in'

d_o_temp.stats_20 = igraph::degree(sub.graphmatr, mode = 'out') %>% tibble::enframe()
d_o_temp.stats_20$full = d_o_bw.g[d_o_temp.stats_20$name]
d_o_temp.stats_20$metric = 'degree-out'

b_temp.stats_20 = igraph::betweenness(sub.graphmatr) %>% tibble::enframe()
b_temp.stats_20$full = b_bw.g[b_temp.stats_20$name]
b_temp.stats_20$metric = 'betweenness'


e_p20 = cor.test(e_temp.stats_20$value, e_temp.stats_20$full, method = 'spearman')$estimate['rho']
e_p50 = cor.test(e_temp.stats_50$value, e_temp.stats_50$full, method = 'spearman')$estimate['rho']

d_p20 = cor.test(d_temp.stats_20$value, d_temp.stats_20$full, method = 'spearman')$estimate['rho']
d_p50 = cor.test(d_temp.stats_50$value, d_temp.stats_50$full, method = 'spearman')$estimate['rho']

d_i_p20 = cor.test(d_i_temp.stats_20$value, d_i_temp.stats_20$full, method = 'spearman')$estimate['rho']
d_i_p50 = cor.test(d_i_temp.stats_50$value, d_i_temp.stats_50$full, method = 'spearman')$estimate['rho']

d_o_p20 = cor.test(d_o_temp.stats_20$value, d_o_temp.stats_20$full, method = 'spearman')$estimate['rho']
d_o_p50 = cor.test(d_o_temp.stats_50$value, d_o_temp.stats_50$full, method = 'spearman')$estimate['rho']

b_p20 = cor.test(b_temp.stats_20$value, b_temp.stats_20$full, method = 'spearman')$estimate['rho']
b_p50 = cor.test(b_temp.stats_50$value, b_temp.stats_50$full, method = 'spearman')$estimate['rho']


e_temp.stats_20 = e_temp.stats_20 %>% mutate(remaining = paste0('20% Remaining ', "(r of ", round(e_p20,3), ", p<.01)")) %>% mutate(remain = '20% Remaining')

e_temp.stats_50 = e_temp.stats_50 %>% mutate(remaining = paste0('50% Remaining ', "(r of ",round(e_p50,3), ", p<.01)")) %>% mutate(remain = '50% Remaining')

d_temp.stats_20 = d_temp.stats_20 %>% mutate(remaining = paste0('20% Remaining ', "(r of ", round(d_p20,3), ", p<.01)")) %>% mutate(remain = '20% Remaining')

d_temp.stats_50 = d_temp.stats_50 %>% mutate(remaining = paste0('50% Remaining ', "(r of ",round(d_p50,3), ", p<.01)")) %>% mutate(remain = '50% Remaining')

d_i_temp.stats_20 = d_i_temp.stats_20 %>% mutate(remaining = paste0('20% Remaining ', "(r of ", round(d_i_p20,3), ", p<.01)")) %>% mutate(remain = '20% Remaining')

d_i_temp.stats_50 = d_i_temp.stats_50 %>% mutate(remaining = paste0('50% Remaining ', "(r of ",round(d_i_p50,3), ", p<.01)")) %>% mutate(remain = '50% Remaining')

d_o_temp.stats_20 = d_o_temp.stats_20 %>% mutate(remaining = paste0('20% Remaining ', "(r of ", round(d_o_p20,3), ", p<.01)")) %>% mutate(remain = '20% Remaining')

d_o_temp.stats_50 = d_o_temp.stats_50 %>% mutate(remaining = paste0('50% Remaining ', "(r of ",round(d_o_p50,3), ", p<.01)")) %>% mutate(remain = '50% Remaining')

b_temp.stats_20 = b_temp.stats_20 %>% mutate(remaining = paste0('20% Remaining ', "(r of ", round(b_p20,3), ", p<.01)")) %>% mutate(remain = '20% Remaining')

b_temp.stats_50 = b_temp.stats_50 %>% mutate(remaining = paste0('50% Remaining ', "(r of ",round(b_p50,3), ", p<.01)")) %>% mutate(remain = '50% Remaining')

a = rbind(e_temp.stats_20, e_temp.stats_50) %>% ggplot() + 
  geom_point(aes(x = value, y = full), size = .5, alpha  = .6) + 
  facet_wrap(metric~remaining, scales = 'free_x') + 
  theme_bw() + 
  theme(text = element_text(family = 'Times')) + 
  labs(x = 'Sample Network', y = 'Full Network' )

b = rbind(
      d_temp.stats_20, d_temp.stats_50) %>% ggplot() + 
  geom_point(aes(x = value, y = full), size = .5, alpha  = .6) + 
  facet_wrap(metric~remaining, scales = 'free_x') + 
  theme_bw() + 
  theme(text = element_text(family = 'Times')) + 
  labs(x = 'Sample Network', y = 'Full Network' )

c = rbind(
      d_i_temp.stats_20, d_i_temp.stats_50) %>% ggplot() + 
  geom_point(aes(x = value, y = full), size = .5, alpha  = .6) + 
  facet_wrap(metric~remaining, scales = 'free_x') + 
  theme_bw() + 
  theme(text = element_text(family = 'Times')) + 
  labs(x = 'Sample Network', y = 'Full Network' )

d = rbind(
      d_o_temp.stats_20, d_o_temp.stats_50) %>% ggplot() + 
  geom_point(aes(x = value, y = full), size = .5, alpha  = .6) + 
  facet_wrap(metric~remaining, scales = 'free_x') + 
  theme_bw() + 
  theme(text = element_text(family = 'Times')) + 
  labs(x = 'Sample Network', y = 'Full Network' )

e = rbind(
      b_temp.stats_20, b_temp.stats_50) %>% ggplot() + 
  geom_point(aes(x = value, y = full), size = .5, alpha  = .6) + 
  facet_wrap(metric~remaining, scales = 'free_x') + 
  theme_bw() + 
  theme(text = element_text(family = 'Times')) + 
  labs(x = 'Sample Network', y = 'Full Network' )

library(cowplot)
plot_grid(a,b,c,d,e, ncol = 1)

```
